{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prepared for the course \"TDPS22: Data Science Programming\" at Jönköping University, Teacher: [Marcel Bollmann](mailto:marcel.bollmann@ju.se)*\n",
    "\n",
    "# Exercise 5: Statistical Modeling & Advanced Topics\n",
    "\n",
    "This notebook contains advanced exercises on machine learning and statistical modeling, focusing on Scikit-learn and Pingouin.  We'll look at three topics in particular: how to determine _feature importance_ in a classifier; how to perform basic statistical modeling and run statistical tests; and how to do dimensionality reduction.\n",
    "\n",
    "For the first parts, we'll continue using the Penguins dataset; for dimensionality reduction, we will look at an example with English word vectors.\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "- Know how to _find and apply functions_ related to statistical modeling, testing, model inspection, and more.\n",
    "- Know how to _perform dimensionality reduction_ of high-dimensional data with Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from rich import load_ipython_extension\n",
    "    %load_ext rich\n",
    "except ImportError:\n",
    "    try:\n",
    "        from rich import pretty\n",
    "        pretty.install()\n",
    "    except ImportError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>Adelie</th>\n",
       "      <th>Chinstrap</th>\n",
       "      <th>Gentoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>47.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>214.0</td>\n",
       "      <td>4925.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>Biscoe</td>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a predefined dataset from Seaborn\n",
    "df = sns.load_dataset(\"penguins\").dropna()\n",
    "df = df.join(pd.get_dummies(df[\"species\"]))  # one-hot encoding for species\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "## Permutation feature importance\n",
    "\n",
    "We'll keep working with the Penguins dataset first – I hope you're not getting bored of these little guys yet!\n",
    "\n",
    "In Exercise 4, we trained a bunch of classifiers where I told you which input features to train on.  The last task we ended with was: *Can we predict the sex of a penguin just from knowing its body mass and species?*  We tried to improve our model, and you probably ended up with a classifier that could do this reasonably well, but not perfectly.\n",
    "\n",
    "Now, we ask a different question: **Which input features are particularly helpful/important for predicting the _sex_ of a penguin?** This is _feature importance._\n",
    "\n",
    "We'll try to implement a particular algorithm for feature importance here, called **permutation feature importance**.  Its core idea is simple: for a given feature (say, _body mass_), shuffle the feature column (so that every penguin randomly gets a _body mass_ from another penguin), and see how the classifier accuracy changes.  If it drops a lot, the feature was probably important; if it stays the same, the feature didn't matter.  This method is attractive because it **_works with any model_** (including neural networks of all kinds) and **_it's fast_** (because it doesn't require retraining any part of the model), so it's really good to know about!\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "For more information about the algorithm:\n",
    "\n",
    "+ [\"Permutation Feature Importance\" from _Interpretable Machine Learning_ by Christoph Molnar](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "\n",
    "For how to implement this in Scikit-learn:\n",
    "\n",
    "+ [`sklearn.inspection.permutation_importance`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance) from the _Scikit-learn documentation_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "Let's start by defining our input and target features, as well as the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X = df[\n",
    "    [\n",
    "        \"bill_length_mm\",\n",
    "        \"bill_depth_mm\",\n",
    "        \"flipper_length_mm\",\n",
    "        \"body_mass_g\",\n",
    "        \"Adelie\",\n",
    "        \"Chinstrap\",\n",
    "        \"Gentoo\",\n",
    "    ]\n",
    "]\n",
    "y = df[\"sex\"]\n",
    "model = SVC(C=1.0, gamma=0.0001, kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Make a train/test split, train the model, and calculate its accuracy on the test set.** This should be familiar from Exercise 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Calculate permutation importance over the input features.** Look at the _mean_ importance across 50 repeats. Try to answer the questions:\n",
    "\n",
    "  - **Which feature is the most important for the classifier?**\n",
    "  - **Which features (if any) are not important at all?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Change your train/test data** (e.g. `X_train`, `X_test`) **to only include the _two most important features_.** Then retrain the model with that & evaluate it again. Did the accuracy score change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Do the same thing, but train the model on the _two least important features only_.** What accuracy score do you expect? See if the result matches your expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What accuracy score would you get if you always guessed the majority class?** In other words, determine which sex (Male or Female) is represented more often in the training set, make a list of \"guesses\" that only contain this majority class label, and calculate its accuracy score on the test set. How does it compare to the accuracy of your classifier from Q4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll come back to these models we trained here in a bit, so keep your predictions around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "## Statistical testing\n",
    "\n",
    "Statistical testing, particularly hypothesis testing, is an important aspect of data-driven research.  You have heard about certain statistical hypothesis tests, such as the _t-test_ the _Wilcoxon signed rank test_, in the \"Evaluation\" lecture of the Data Science course. You will also likely hear some more about it in the Research Methods course. We don't have time to go into details about _when to select which statistical test and why_, so we will focus on _how to run_ such tests here.\n",
    "\n",
    "As mentioned in the lecture, there are _many_ ways to do statistical testing and modeling in Python. You will often see people using [the `scipy.stats` module](https://docs.scipy.org/doc/scipy/reference/stats.html) or [the `statsmodels` library](https://www.statsmodels.org/stable/index.html), but we will mainly look at [Pingouin](https://pingouin-stats.org/) here, as it is extremely simple to use & get started with.\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "+ [Pingouin Quick Start](https://pingouin-stats.org/index.html#quick-start) _(take a look at the \"10 minutes to Pingouin\" section!)_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a subset of our dataset, and look at **female Gentoo penguins** only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df[(df[\"species\"] == \"Gentoo\") & (df[\"sex\"] == \"Female\")].drop(\n",
    "    [\"Adelie\", \"Chinstrap\", \"Gentoo\"], axis=1  # drop the one-hot columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Which of our measurement variables are _normally distributed_?** Checking for a normal distribution is useful, because it is often a requirement for using other metrics that the data is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Which of the measurement variables are _correlated_?** Perform a pairwise correlation analysis with all the columns of our dataframe. Use Pearson correlation if the variables are normally distributed, and Spearman correlation if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Make a new Pandas DataFrame with \"classifier correctness\" labels.** This DataFrame should have:\n",
    "\n",
    "- One **column per classifier**, e.g. the \"good\" classifier from Q3, the \"bad\" classifier from Q4, and the \"majority\" classifier from Q5.\n",
    "- One **row per instance in your test set**.\n",
    "- A value of **`True` or `False` in each cell**, depending on whether the classifier _(column)_ correctly classified the instance _(row)_ or not.\n",
    "\n",
    "Your resulting DataFrame could look something like this:\n",
    "\n",
    "|       | **good** | **bad**  | **majority** |\n",
    "|-------|----------|----------|--------------|\n",
    "| **0** | True     | False    | False        |\n",
    "| **1** | True     | False    | False        |\n",
    "| **2** | True     | True     | True         |\n",
    "| ...   | ...      | ...      | ...          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Is the \"bad\" model (from Q4) still significantly better than the \"majority\" class model (from Q5)?**\n",
    "\n",
    "We want to answer this question by using **McNemar's test,** using a significance level of $0.05$.  This means that if McNemar's test gives us a $p$-value $< 0.05$, we accept that there is a significant difference between our two classifiers.  If you use [Pingouin's implementation of McNemar's test](https://pingouin-stats.org/generated/pingouin.chi2_mcnemar.html), you will find that the DataFrame from Q6 is exactly what we need to use with Pingouin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Is the \"good\" model (from Q3) significantly better than the \"bad\" model (from Q4)?**  Same as above, just comparing different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "## Dimensionality reduction\n",
    "\n",
    "So far, our data only had a few variables at a time. For high-dimensional data, e.g. weights learned by a neural network, _dimensionality reduction_ techniques can come in useful to perform analyses. Here, we look at two such techniques–**principal component analysis (PCA)** and **t-distributed stochastic neighbor embedding (t-SNE)**–and how to perform them in Python.\n",
    "\n",
    "For this, we'll load another dataset that comes from a neural network: a small subset of the [GloVe pre-trained word vectors for English](https://nlp.stanford.edu/projects/glove/). It contains vector representations of almost 3,000 English words where each vector has a dimensionality of 50. _(The full GloVe dataset covers 400,000 words and offers up to 300-dimensional vectors.)_ Our goal is to learn something about what these vectors actually encode.\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "+ [\"PCA as dimensionality reduction\"](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html#PCA-as-dimensionality-reduction) from the _Python Data Science Handbook_\n",
    "+ [How to use t-SNE effectively](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "Another useful library for this kind of task is [HyperTools](https://github.com/ContextLab/hypertools), which I don't discuss here, but you may of course also use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.217050</td>\n",
       "      <td>0.465150</td>\n",
       "      <td>-0.467570</td>\n",
       "      <td>0.10082</td>\n",
       "      <td>1.013500</td>\n",
       "      <td>0.74845</td>\n",
       "      <td>-0.531040</td>\n",
       "      <td>-0.262560</td>\n",
       "      <td>0.168120</td>\n",
       "      <td>0.131820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138130</td>\n",
       "      <td>0.369730</td>\n",
       "      <td>-0.642890</td>\n",
       "      <td>0.024142</td>\n",
       "      <td>-0.039315</td>\n",
       "      <td>-0.260370</td>\n",
       "      <td>0.12017</td>\n",
       "      <td>-0.043782</td>\n",
       "      <td>0.410130</td>\n",
       "      <td>0.17960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0.456430</td>\n",
       "      <td>-0.501790</td>\n",
       "      <td>0.082902</td>\n",
       "      <td>-0.72677</td>\n",
       "      <td>-0.362920</td>\n",
       "      <td>-0.02104</td>\n",
       "      <td>-0.167750</td>\n",
       "      <td>0.625280</td>\n",
       "      <td>-0.259540</td>\n",
       "      <td>0.281720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180080</td>\n",
       "      <td>-0.921530</td>\n",
       "      <td>0.381290</td>\n",
       "      <td>0.023647</td>\n",
       "      <td>0.375840</td>\n",
       "      <td>-0.057270</td>\n",
       "      <td>-0.30060</td>\n",
       "      <td>0.383230</td>\n",
       "      <td>-0.355470</td>\n",
       "      <td>-0.14419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>0.135230</td>\n",
       "      <td>-0.241440</td>\n",
       "      <td>0.584420</td>\n",
       "      <td>-0.15259</td>\n",
       "      <td>0.528840</td>\n",
       "      <td>0.23586</td>\n",
       "      <td>0.441880</td>\n",
       "      <td>0.366560</td>\n",
       "      <td>0.827030</td>\n",
       "      <td>0.750210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060980</td>\n",
       "      <td>-0.125100</td>\n",
       "      <td>-0.052429</td>\n",
       "      <td>0.444010</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>-0.211810</td>\n",
       "      <td>0.44934</td>\n",
       "      <td>0.385050</td>\n",
       "      <td>-0.354570</td>\n",
       "      <td>0.20372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.864540</td>\n",
       "      <td>-0.390890</td>\n",
       "      <td>0.980690</td>\n",
       "      <td>-0.43311</td>\n",
       "      <td>0.544040</td>\n",
       "      <td>-0.31648</td>\n",
       "      <td>-0.321860</td>\n",
       "      <td>0.832060</td>\n",
       "      <td>-0.053605</td>\n",
       "      <td>0.100040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050596</td>\n",
       "      <td>-0.215260</td>\n",
       "      <td>0.402560</td>\n",
       "      <td>0.326370</td>\n",
       "      <td>0.069126</td>\n",
       "      <td>-0.048120</td>\n",
       "      <td>0.17884</td>\n",
       "      <td>-0.138790</td>\n",
       "      <td>-0.225270</td>\n",
       "      <td>0.23315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abortion</th>\n",
       "      <td>-0.150790</td>\n",
       "      <td>-0.330830</td>\n",
       "      <td>-0.494370</td>\n",
       "      <td>-1.00370</td>\n",
       "      <td>-0.498520</td>\n",
       "      <td>2.27160</td>\n",
       "      <td>-0.110920</td>\n",
       "      <td>-0.703130</td>\n",
       "      <td>0.581470</td>\n",
       "      <td>0.446610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407510</td>\n",
       "      <td>0.212580</td>\n",
       "      <td>-0.025393</td>\n",
       "      <td>1.659600</td>\n",
       "      <td>0.322240</td>\n",
       "      <td>0.100850</td>\n",
       "      <td>-0.57467</td>\n",
       "      <td>-0.172060</td>\n",
       "      <td>-0.052021</td>\n",
       "      <td>0.76517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>-0.029163</td>\n",
       "      <td>0.817690</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>-0.77857</td>\n",
       "      <td>1.104900</td>\n",
       "      <td>-0.13655</td>\n",
       "      <td>-0.024691</td>\n",
       "      <td>-0.051103</td>\n",
       "      <td>0.779500</td>\n",
       "      <td>0.051357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686590</td>\n",
       "      <td>-0.302940</td>\n",
       "      <td>-0.551750</td>\n",
       "      <td>0.964660</td>\n",
       "      <td>0.053103</td>\n",
       "      <td>-0.084807</td>\n",
       "      <td>0.85120</td>\n",
       "      <td>-0.541860</td>\n",
       "      <td>0.324530</td>\n",
       "      <td>0.58425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yours</th>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.181130</td>\n",
       "      <td>0.791160</td>\n",
       "      <td>-0.39936</td>\n",
       "      <td>0.644360</td>\n",
       "      <td>-0.25559</td>\n",
       "      <td>0.198670</td>\n",
       "      <td>0.145580</td>\n",
       "      <td>-0.153710</td>\n",
       "      <td>0.568930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150870</td>\n",
       "      <td>-1.105800</td>\n",
       "      <td>-0.013501</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>-0.225070</td>\n",
       "      <td>0.313430</td>\n",
       "      <td>-0.69607</td>\n",
       "      <td>-0.590710</td>\n",
       "      <td>-0.162260</td>\n",
       "      <td>0.82532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yourself</th>\n",
       "      <td>-0.125310</td>\n",
       "      <td>-0.410110</td>\n",
       "      <td>0.304610</td>\n",
       "      <td>-0.93852</td>\n",
       "      <td>0.765590</td>\n",
       "      <td>-0.12585</td>\n",
       "      <td>0.203160</td>\n",
       "      <td>0.338310</td>\n",
       "      <td>0.133250</td>\n",
       "      <td>0.543080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349380</td>\n",
       "      <td>-0.791710</td>\n",
       "      <td>0.114520</td>\n",
       "      <td>0.899400</td>\n",
       "      <td>0.266480</td>\n",
       "      <td>0.092037</td>\n",
       "      <td>0.37759</td>\n",
       "      <td>-0.420590</td>\n",
       "      <td>0.578560</td>\n",
       "      <td>0.69841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>-0.939750</td>\n",
       "      <td>0.772420</td>\n",
       "      <td>-0.839690</td>\n",
       "      <td>-0.58010</td>\n",
       "      <td>0.227820</td>\n",
       "      <td>-0.20413</td>\n",
       "      <td>-0.430930</td>\n",
       "      <td>-0.193480</td>\n",
       "      <td>-0.110530</td>\n",
       "      <td>0.203460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472800</td>\n",
       "      <td>-0.795680</td>\n",
       "      <td>-0.376570</td>\n",
       "      <td>-0.207750</td>\n",
       "      <td>0.728660</td>\n",
       "      <td>-0.082940</td>\n",
       "      <td>-0.52826</td>\n",
       "      <td>-0.373640</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>-0.13457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>-0.345210</td>\n",
       "      <td>-0.051935</td>\n",
       "      <td>0.070193</td>\n",
       "      <td>0.64053</td>\n",
       "      <td>-0.021476</td>\n",
       "      <td>-1.13200</td>\n",
       "      <td>-0.204020</td>\n",
       "      <td>-0.519040</td>\n",
       "      <td>0.451570</td>\n",
       "      <td>-1.091700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.456080</td>\n",
       "      <td>0.024629</td>\n",
       "      <td>0.810070</td>\n",
       "      <td>-0.519290</td>\n",
       "      <td>-0.672930</td>\n",
       "      <td>-0.414600</td>\n",
       "      <td>0.67557</td>\n",
       "      <td>0.219900</td>\n",
       "      <td>0.153490</td>\n",
       "      <td>-0.13133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2947 rows × 50 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = pd.read_csv(\"data/glove.subset.50d.txt\", sep=\" \", header=None, index_col=0)\n",
    "glove.index.name = \"word\"\n",
    "glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Perform a principal component analysis (PCA) to map the GloVe vectors into 2-dimensional space.** Two dimensions are a common choice to make them suitable for visualization.\n",
    "\n",
    "If you are using Scikit-learn to do this, you will find that the 2-dimensional vectors are returned as a NumPy array.\n",
    "\n",
    "- **Also merge this back into a DataFrame that has the actual words as an index again**, just like above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Plot some 2-dimensional word vectors using the following function.**  It defines some words in some categories, takes _the first two dimensions_ and plots them as a scatterplot, along with labels of the actual words.\n",
    "\n",
    "If you created your DataFrame in Q10 correctly, you should just need to call this function on your transformed DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(df):\n",
    "    \"\"\"This function takes a DataFrame with a word index\n",
    "    and plots some chosen example words of different categories.\"\"\"\n",
    "\n",
    "    word_cats = {\n",
    "        \"food\": [\"apple\", \"orange\", \"lemon\", \"tomato\", \"potato\", \"chocolate\", \"bread\", \"butter\"],\n",
    "        \"transport\": [\"car\", \"train\", \"rail\", \"plane\", \"bike\", \"boat\", \"ship\", \"foot\"],\n",
    "        \"pronouns\": [\"he\", \"she\", \"it\", \"they\", \"them\", \"you\", \"your\", \"us\"],\n",
    "    }\n",
    "    fig, ax = plt.subplots()\n",
    "    pal = sns.color_palette(\"bright\")\n",
    "    for i, (category, words) in enumerate(word_cats.items()):\n",
    "        for word in words:\n",
    "            pos = df.loc[word].iloc[0], df.loc[word].iloc[1]\n",
    "            ax.scatter(*pos, color=pal[i])\n",
    "            ax.annotate(word, pos, color=pal[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. Perform a t-SNE transformation and plot it as well!**  Scikit-learn also provides this algorithm as [`sklearn.manifold.TSNE`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), and you can use it exactly the same way as PCA. This is both powerful and dangerous – you don't really need to \"know\" anything about the algorithm to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. Repeat the t-SNE plot with different random states and perplexity values.** t-SNE is a very powerful and useful technique, but it is sensitive to hyperparameters. I strongly recommend you to read [How to use t-SNE effectively](https://distill.pub/2016/misread-tsne/) if you ever want to use it in practice.\n",
    "\n",
    "For now, let's just see what happens if we change the `random_state` or the `perplexity` (the default is 30, and the authors recommend to try values between 5 and 50.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-dsp",
   "language": "python",
   "name": "python3-dsp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
